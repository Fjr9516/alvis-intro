{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data with PyTorch\n",
    "In this notebook we will investigate a few different ways to handle data with PyTorch on Alvis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your own data\n",
    "In many cases you have a dataset in mind that you've already acquired and are keeping in your home folder or perhaps more probable in a storage project.\n",
    "\n",
    "When it comes to using datasets in training datasets the most efficient approach that we have found to work on Alvis is to use utilities to directly stream data from uncompressed tar-archives or zip-archives (though highly compressed zip files can also sometimes be slow).\n",
    "\n",
    "In this section we will use the tiny-ImageNet dataset in `/mimer/NOBACKUP/Datasets` but with the hope that you can adapt it to any dataset that you have in your project storage. First let us take a look at the dataset.\n",
    "\n",
    "### Investigating the contents\n",
    "Let's take a look at what is contained in this archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import FileOpener\n",
    "\n",
    "# Look at the first five files to get a sense of the structure\n",
    "path_to_dataset = '/mimer/NOBACKUP/Datasets/tiny-imagenet-200/tiny-imagenet-200.zip'\n",
    "fileopener = FileOpener([path_to_dataset], mode='b')\n",
    "for ix, (filename, stream) in enumerate(fileopener.load_from_zip()):\n",
    "    print(filename)\n",
    "    if ix >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:*** Investigating files like this can be quite slow if the archives are very large. Looking at the first few files are fast and can be good to get a sense of the file, but you don't want to have to search through them every time. If there is a README in connection with the dataset it is wise to take a look at it. Furthermore, you might want to note down the structure inside the archive yourself if it isn't in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at one of the txt files next\n",
    "datapipe = fileopener.load_from_zip()\n",
    "datapipe = datapipe.filter(lambda d: d[0].endswith('wnids.txt'))\n",
    "for fn, txtfile in datapipe:\n",
    "    print(fn)\n",
    "    wnids = txtfile.read().decode('utf-8').split()\n",
    "    print(wnids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will later be used as the labels for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have used built in utilties or shell utlities for this as well. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "ziphandle = zipfile.ZipFile(path_to_dataset)\n",
    "\n",
    "# Count the number of training files in the dataset\n",
    "len([fn for fn in ziphandle.namelist() if 'train' in fn and fn.endswith('.JPEG')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fast for zip-files as they have information about all of its members easily retriveable for a tarfile you would have to traverse the entire archive (with e.g. `tarfile.TarFile.getmembers`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnmatch import fnmatch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "# Construct datapipe to load images\n",
    "datapipe = fileopener.load_from_zip()\n",
    "def train_image_filter(datum):\n",
    "    filename, _ = datum\n",
    "    return fnmatch(filename, '*train*.JPEG')\n",
    "datapipe = datapipe.filter(train_image_filter)\n",
    "\n",
    "\n",
    "# Visualize images\n",
    "fig, ax_grid = plt.subplots(3, 3, figsize=(15, 15))\n",
    "for ax, (fn, stream) in zip(ax_grid.flatten(), datapipe):\n",
    "    # Get path to file and label\n",
    "    label = fn.split(\"/\")[-1].split('_')[0]\n",
    "    \n",
    "    # Add to axis\n",
    "    img = plt.imread(stream)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Label {label}')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be worth noting that the image labels are listed in wnids.txt that can be found in the archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier from this data\n",
    "Now we have some understanding of what the database does and we are ready to do some ML on it.\n",
    "\n",
    "First we will define our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use torch.hub to load a pretrained model\n",
    "efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "#preprocessing_utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
    "\n",
    "# We freeze all parameters except the last layer\n",
    "freeze_blocks = [efficientnet.stem] + [\n",
    "    layer\n",
    "    for layer in efficientnet.layers\n",
    "    if layer != efficientnet.classifier\n",
    "]\n",
    "for block in freeze_blocks:\n",
    "    for parameter in block:\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "# Modify the number of output classes to 200\n",
    "efficientnet.classifier.fc = nn.Linear(efficientnet.classifier.fc.in_features, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnet\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will construct the dataloader from a datapipe. Compared to previous datapipes we will also add:\n",
    " - possibility to shuffle data\n",
    " - at the end construct batchable tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchdata.datapipes import functional_datapipe\n",
    "from torchdata.datapipes.iter import IterDataPipe\n",
    "from torch.utils.data.backward_compatibility import worker_init_fn\n",
    "\n",
    "# Construct datapipe to load images\n",
    "datapipe = fileopener.load_from_zip()\n",
    "def train_image_filter(datum):\n",
    "    '''Filter for .JPEG in train folder'''\n",
    "    filename, _ = datum\n",
    "    wnid = fn.split(\"/\")[-1].split('_')[0]\n",
    "    return fnmatch(filename, '*train*.JPEG') and (wnid in wnids)\n",
    "datapipe = datapipe.filter(train_image_filter)\n",
    "datapipe = datapipe.shuffle()  # active iff dataloader(shuffle=True)\n",
    "\n",
    "# For use with multiple workers specify which workers uses which data\n",
    "datapipe = datapipe.sharding_filter()\n",
    "\n",
    "# Convert stream to image tensor and label\n",
    "wnid2label = {wnid: label for label, wnid in enumerate(wnids)}\n",
    "def parse_tiny_imagenet(datum):\n",
    "    '''Parse filename and image stream into label and image tensors'''\n",
    "    fn, stream = datum\n",
    "    \n",
    "    # Get label from filename\n",
    "    wnid = fn.split(\"/\")[-1].split('_')[0]\n",
    "    label = wnid2label[wnid]\n",
    "\n",
    "    # Parse image into Tensor of size (Channel, Px, Py)\n",
    "    img_array = np.array(Image.open(stream))\n",
    "    if img_array.ndim < 3:\n",
    "        # Greyscale to RGB\n",
    "        img_array = np.repeat(img_array[..., np.newaxis], 3, -1)\n",
    "\n",
    "    img_tensor = torch.from_numpy(img_array)\n",
    "    img_tensor = img_tensor.permute(2,0,1)\n",
    "    return img_tensor.float(), label\n",
    "datapipe = datapipe.map(parse_tiny_imagenet)\n",
    "\n",
    "# Manually set length (will not affect how many elements that can be yielded)\n",
    "# in future use https://pytorch.org/data/main/generated/torchdata.datapipes.iter.LengthSetter.html\n",
    "if not hasattr(datapipe, 'set_length'):\n",
    "    @functional_datapipe('set_length')\n",
    "    class LengthSetterIterDataPipe(IterDataPipe):\n",
    "        def __init__(self, source_datapipe: IterDataPipe, length: int):\n",
    "            self.source_datapipe = source_datapipe\n",
    "            assert length >= 0\n",
    "            self.length = length\n",
    "\n",
    "        def __iter__(self):\n",
    "            yield from self.source_datapipe\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            return self.length\n",
    "datapipe = datapipe.set_length(100_000)\n",
    "\n",
    "# Construct dataloader from datapipe\n",
    "dataloader = DataLoader(\n",
    "    datapipe,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    worker_init_fn=worker_init_fn,  # in connection with sharding_filter and multiple workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(\n",
    "    dataloader,\n",
    "    model,\n",
    "    opt,\n",
    "    loss_func,\n",
    "    n_epochs=3,\n",
    "    device=torch.device(\"cuda:0\"),\n",
    "):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        loss_sum = 0.0\n",
    "        n_correct = 0\n",
    "        for i_batch, (x, label) in enumerate(dataloader):\n",
    "            print('\\r' + f'Batch: {i_batch}/{len(dataloader)}', end='')\n",
    "            x, label = x.to(device), label.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = loss_func(logits, label)\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "            n_correct += (logits.argmax(1) == label).long().sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        avg_loss = loss_sum / (i_batch + 1)\n",
    "        accuracy = n_correct / len(dataloader.dataset)\n",
    "        print(f\" Loss: {avg_loss}\", f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(dataloader, model, opt, loss_func, n_epochs=3);\n",
    "# If you get an error message \"Unable to find a valid cuDNN algorithm\"\n",
    "# then, you're probably running out of GPU memory and should kill other\n",
    "# processes using up memory and/or reduce the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    " 1. Make yourself acquainted with the above code.\n",
    " 2. Take a look at `jobscript-pytorch.sh` to see how you would go about training something non-interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using available datasets\n",
    "Some common public datasets are available at `/cephyr/NOBACKUP/Datasets`, if there are some specific dataset you would like to see added you can create a request through [support](https://supr.snic.se/support/).\n",
    "\n",
    "In this part we will access the processed MNIST dataset available at `/cephyr/NOBACKUP/Datasets/MNIST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 (3, 3) convolutional filters followed by a dense layer\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 10, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(6760, 10),\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it is really simple as this dataset has been processed for use with `torchvision.datasets.MNIST` and all we need to do is supply the correct path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "dataset = datasets.MNIST(\"/cephyr/NOBACKUP/Datasets\", transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataloader, model, opt, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data through the torchvision API\n",
    "At `torchvision.datasets`, `torchaudio.datasets` and `torchtext.datasets` all have similar APIs that can be used to download datasets that do not exist in `/cephyr/NOBACKUP/Datasets`. However, note that this can take some time and you will have to store them yourself. If you are interested in a dataset that permit us redistributing it, then contact us through the regular support form and we can look at storing it centrally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
