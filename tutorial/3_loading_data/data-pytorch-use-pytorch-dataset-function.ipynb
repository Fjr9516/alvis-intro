{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data with PyTorch\n",
    "In this notebook we will investigate a few different ways to handle data with PyTorch on Alvis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your own data\n",
    "In many cases you have a dataset in mind that you've already acquired and are keeping in your home folder or perhaps more probable in a storage project.\n",
    "\n",
    "When it comes to using datasets in training datasets the most efficient approach that we have found to work on Alvis is to use utilities to directly stream data from uncompressed tar-archives or zip-archives (though highly compressed zip files can also sometimes be slow).\n",
    "\n",
    "In this section we will use the tiny-ImageNet dataset in `/mimer/NOBACKUP/Datasets` but with the hope that you can adapt it to any dataset that you have in your project storage. First let us take a look at the dataset.\n",
    "\n",
    "### Investigating the contents\n",
    "Let's take a look at what is contained in this archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_dataset import (\n",
    "    PATH_TO_DATASET,\n",
    "    examine_zipfile_structure,\n",
    "    read_labels_from_txt,\n",
    "    count_train_images,\n",
    "    visualize_sample_images,\n",
    "    TinyImageNetDataset\n",
    ")\n",
    "\n",
    "def get_dataloader(path_to_dataset: str, batch_size: int = 64, shuffle=True, num_workers: int = 4):\n",
    "    \"\"\"Returns a DataLoader for the TinyImageNet dataset.\"\"\"\n",
    "    dataset = TinyImageNetDataset(path_to_dataset, split=\"train\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the zipfile 120609\n",
      "tiny-imagenet-200/\n",
      "tiny-imagenet-200/words.txt\n",
      "tiny-imagenet-200/wnids.txt\n",
      "tiny-imagenet-200/test/\n",
      "tiny-imagenet-200/test/images/\n",
      "tiny-imagenet-200/test/images/test_1860.JPEG\n",
      "tiny-imagenet-200/test/images/test_613.JPEG\n",
      "...\n",
      "tiny-imagenet-200/val/images/val_9872.JPEG\n",
      "tiny-imagenet-200/val/images/val_2584.JPEG\n",
      "tiny-imagenet-200/val/images/val_5908.JPEG\n"
     ]
    }
   ],
   "source": [
    "# Investigate the contents\n",
    "# Look at the structure of the zipfile to understand its contents.\n",
    "# This gives an overview of the file organization within the archive.\n",
    "examine_zipfile_structure(PATH_TO_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:*** Investigating files like this can be quite slow if the archives are very large. Looking at the first few files are fast and can be good to get a sense of the file, but you don't want to have to search through them every time. If there is a README in connection with the dataset it is wise to take a look at it. Furthermore, you might want to note down the structure inside the archive yourself if it isn't in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['n02124075', 'n04067472', 'n04540053', 'n04099969', 'n07749582', 'n01641577', 'n02802426', 'n09246464', 'n07920052', 'n03970156', 'n03891332', 'n02106662', 'n03201208', 'n02279972', 'n02132136', 'n04146614', 'n07873807', 'n02364673', 'n04507155', 'n03854065', 'n03838899', 'n03733131', 'n01443537', 'n07875152', 'n03544143', 'n09428293', 'n03085013', 'n02437312', 'n07614500', 'n03804744', 'n04265275', 'n02963159', 'n02486410', 'n01944390', 'n09256479', 'n02058221', 'n04275548', 'n02321529', 'n02769748', 'n02099712', 'n07695742', 'n02056570', 'n02281406', 'n01774750', 'n02509815', 'n03983396', 'n07753592', 'n04254777', 'n02233338', 'n04008634', 'n02823428', 'n02236044', 'n03393912', 'n07583066', 'n04074963', 'n01629819', 'n09332890', 'n02481823', 'n03902125', 'n03404251', 'n09193705', 'n03637318', 'n04456115', 'n02666196', 'n03796401', 'n02795169', 'n02123045', 'n01855672', 'n01882714', 'n02917067', 'n02988304', 'n04398044', 'n02843684', 'n02423022', 'n02669723', 'n04465501', 'n02165456', 'n03770439', 'n02099601', 'n04486054', 'n02950826', 'n03814639', 'n04259630', 'n03424325', 'n02948072', 'n03179701', 'n03400231', 'n02206856', 'n03160309', 'n01984695', 'n03977966', 'n03584254', 'n04023962', 'n02814860', 'n01910747', 'n04596742', 'n03992509', 'n04133789', 'n03937543', 'n02927161', 'n01945685', 'n02395406', 'n02125311', 'n03126707', 'n04532106', 'n02268443', 'n02977058', 'n07734744', 'n03599486', 'n04562935', 'n03014705', 'n04251144', 'n04356056', 'n02190166', 'n03670208', 'n02002724', 'n02074367', 'n04285008', 'n04560804', 'n04366367', 'n02403003', 'n07615774', 'n04501370', 'n03026506', 'n02906734', 'n01770393', 'n04597913', 'n03930313', 'n04118538', 'n04179913', 'n04311004', 'n02123394', 'n04070727', 'n02793495', 'n02730930', 'n02094433', 'n04371430', 'n04328186', 'n03649909', 'n04417672', 'n03388043', 'n01774384', 'n02837789', 'n07579787', 'n04399382', 'n02791270', 'n03089624', 'n02814533', 'n04149813', 'n07747607', 'n03355925', 'n01983481', 'n04487081', 'n03250847', 'n03255030', 'n02892201', 'n02883205', 'n03100240', 'n02415577', 'n02480495', 'n01698640', 'n01784675', 'n04376876', 'n03444034', 'n01917289', 'n01950731', 'n03042490', 'n07711569', 'n04532670', 'n03763968', 'n07768694', 'n02999410', 'n03617480', 'n06596364', 'n01768244', 'n02410509', 'n03976657', 'n01742172', 'n03980874', 'n02808440', 'n02226429', 'n02231487', 'n02085620', 'n01644900', 'n02129165', 'n02699494', 'n03837869', 'n02815834', 'n07720875', 'n02788148', 'n02909870', 'n03706229', 'n07871810', 'n03447447', 'n02113799', 'n12267677', 'n03662601', 'n02841315', 'n07715103', 'n02504458']\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at one of the txt files next\n",
    "labels = read_labels_from_txt(PATH_TO_DATASET)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will later be used as the labels for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the number of train files like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 100000\n"
     ]
    }
   ],
   "source": [
    "num_train_images = count_train_images(PATH_TO_DATASET)\n",
    "print(f\"Number of training images: {num_train_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading file information from zip-files is fast as they have information about all of its members easily retriveable. For a tarfile you would have to traverse the entire archive (with e.g. `tarfile.TarFile.getmembers`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "# Displays a 3x3 grid of sample images from the training set.\n",
    "visualize_sample_images(PATH_TO_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be worth noting that the image labels are listed in wnids.txt that can be found in the archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier from this data\n",
    "Now we have some understanding of what the database does and we are ready to do some ML on it.\n",
    "\n",
    "First we will define our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /cephyr/users/klim/Alvis/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/cephyr/users/klim/Alvis/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "/cephyr/users/klim/Alvis/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We will use torch.hub to load a pretrained model\n",
    "efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "#preprocessing_utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
    "\n",
    "# We freeze all parameters except the last layer\n",
    "freeze_blocks = [efficientnet.stem] + [\n",
    "    layer for layer in efficientnet.layers if layer != efficientnet.classifier\n",
    "]\n",
    "for block in freeze_blocks:\n",
    "    for parameter in block:\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "# Modify the number of output classes to 200\n",
    "efficientnet.classifier.fc = nn.Linear(efficientnet.classifier.fc.in_features, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnet\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "loss_func = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize DataLoader using TinyImageNetDataset\n",
    "- Constructs a DataLoader for the TinyImageNet training dataset.\n",
    "- This will be used to iterate through the data in batches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(PATH_TO_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the training function\n",
    "- Implements the main training loop. It iterates over the dataset for multiple epochs,\n",
    "- calculates the loss, and updates the model parameters. Tracks loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, opt, loss_func, n_epochs=3, device=torch.device(\"cuda:0\")):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_sum = 0.0\n",
    "        n_correct = 0\n",
    "        for i_batch, (x, label) in enumerate(dataloader):\n",
    "            print(f'\\rBatch: {i_batch}/{len(dataloader)}', end='')\n",
    "            x, label = x.to(device), label.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = loss_func(logits, label)\n",
    "            loss_sum += loss.item()\n",
    "            n_correct += (logits.argmax(1) == label).long().sum()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        avg_loss = loss_sum / (i_batch + 1)\n",
    "        accuracy = n_correct / len(dataloader.dataset)\n",
    "        print(f\" Loss: {avg_loss}\", f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the training\n",
    "- Executes the training function for 3 epochs.\n",
    "- This will output the loss and accuracy per epoch to track model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1562/1563 Loss: 3.1520847374250396 Accuracy: 0.26718997955322266\n",
      "Batch: 1562/1563 Loss: 2.4606427144943255 Accuracy: 0.39757999777793884\n",
      "Batch: 1562/1563 Loss: 2.19449408589764 Accuracy: 0.45181000232696533\n",
      "CPU times: user 2min 47s, sys: 4.35 s, total: 2min 51s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(dataloader, model, opt, loss_func, n_epochs=3)\n",
    "# If you get an error message \"Unable to find a valid cuDNN algorithm\"\n",
    "# then, you're probably running out of GPU memory and should kill other\n",
    "# processes using up memory and/or reduce the batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    " 1. Make yourself acquainted with the above code.\n",
    " 2. Take a look at `jobscript-pytorch.sh` to see how you would go about training something non-interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using available datasets\n",
    "Some common public datasets are available at `/mimer/NOBACKUP/Datasets`, if there are some specific dataset you would like to see added you can create a request through [support](https://supr.naiss.se/support/).\n",
    "\n",
    "In this part we will access the processed MNIST dataset available at `/mimer/NOBACKUP/Datasets/MNIST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  (3): Linear(in_features=6760, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 10 (3, 3) convolutional filters followed by a dense layer\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 10, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(6760, 10),\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it is really simple as this dataset has been processed for use with `torchvision.datasets.MNIST` and all we need to do is supply the correct path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "dataset = datasets.MNIST(\"/mimer/NOBACKUP/Datasets\", transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 468/469 Loss: 0.19284047612122127 Accuracy: 0.942383348941803\n",
      "Batch: 468/469 Loss: 0.07343067713518704 Accuracy: 0.9773333668708801\n",
      "Batch: 468/469 Loss: 0.0519570899280364 Accuracy: 0.9830833673477173\n"
     ]
    }
   ],
   "source": [
    "train(dataloader, model, opt, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data through the torchvision API\n",
    "At `torchvision.datasets`, `torchaudio.datasets` and `torchtext.datasets` all have similar APIs that can be used to download datasets that do not exist in `/mimer/NOBACKUP/Datasets`. However, note that this can take some time and you will have to store them yourself. If you are interested in a dataset that permit us redistributing it, then contact us through the regular support form and we can look at storing it centrally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
