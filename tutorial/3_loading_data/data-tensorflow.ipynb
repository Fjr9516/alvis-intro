{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data with TensorFlow\n",
    "In this notebook we will investigate a few different ways to handle data with TensorFlow on Alvis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your own data\n",
    "In many cases you have a dataset in mind that you've already acquired and are keeping in your home folder or perhaps more probable in a storage project.\n",
    "\n",
    "In this section we will use the dataset in `data.tar.gz`, first let us take a look at it.\n",
    "\n",
    "***N.B.:*** We've found that that fastest way to load data on Alvis is to directly stream from archives stored on Mimer. Utilities exist in tensorflow.datasets for zip and tar, but loading from tfrecords might work just as well if not better.\n",
    "\n",
    "### The file tree\n",
    "First we inspect the dataset archive that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# First we will decompress the dataset for better reading\n",
    "if [ ! -f data.tar ]; then\n",
    "    gunzip data.tar.gz\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to iterate over dataset\n",
    "import tarfile\n",
    "import random\n",
    "from matplotlib.image import imread\n",
    "def iter_archive(path, extractfile=False, shuffle=False):\n",
    "    with tarfile.open('data.tar') as datatar:\n",
    "        members = datatar.getmembers()\n",
    "        if shuffle:\n",
    "            random.shuffle(members)\n",
    "        for member in members:\n",
    "            filename = member.name\n",
    "            if not filename.endswith(\".png\"):\n",
    "                continue\n",
    "            if extractfile:\n",
    "                img = imread(datatar.extractfile(member))\n",
    "            else:\n",
    "                img = None\n",
    "            yield filename, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will find the directories and files that do not have names ending with .png\n",
    "# and then count the number of files with names containing \".png\" for each of these\n",
    "import os\n",
    "import tarfile\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five files in the archive\n",
    "for i, (filename, _) in enumerate(iter_archive('data.tar')):\n",
    "    print(filename)\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each directory print the number of files with specific extension\n",
    "directory_entries = Counter()\n",
    "for filename, _ in tqdm(iter_archive('data.tar')):\n",
    "    dirname = os.path.dirname(filename)\n",
    "    extension = os.path.splitext(filename)[-1]\n",
    "    directory_entries.update([f\"{dirname}/*{extension}\"])\n",
    "    while '/' in dirname:\n",
    "        dirname = os.path.dirname(dirname)\n",
    "        directory_entries.update([f\"{dirname}/**/*{extension}\"])\n",
    "\n",
    "for path, entry_count in sorted(directory_entries.items(), key=lambda t:t[1], reverse=True):\n",
    "    print(path, entry_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:*** For this tar file there where \"only\" 60000 files, for archives that are much larger these operations will mean a significant FileIO and should be avoided as much as possible. If there is a README in connection with the dataset it is wise to take a look at it. There are some advantages when it comes to zipfiles as then it is possible to learn about the members of the archive without going through the whole archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at some of the data\n",
    "Now we know the file structure of the data. Let us now get acquainted with the data a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at a few of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnmatch import fnmatch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Visualize images\n",
    "fig, ax_grid = plt.subplots(3, 3, figsize=(15, 15))\n",
    "for ax, (fn, img) in zip(ax_grid.flatten(), iter_archive('data.tar', extractfile=True)):\n",
    "    # Get path to file and label\n",
    "    label = fn.split('/')[1]\n",
    "\n",
    "    # Add to axis\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Label {label}')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the labels are offset by 1 compared to the digits. The dataset is actually a modified version of the MNIST handwritten digit training database. The images have been shrunk to only 9x9 pixels and monochrome images to reduce the size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier from this data\n",
    "Now we have some understanding of what the database does and we are ready to do some ML on it.\n",
    "\n",
    "First we will define our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 (3, 3) convolutional filters followed by a dense layer\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(10, 3, activation=\"relu\", input_shape=(10, 10, 1), use_bias=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the step were we will load the data. When we have a dataset with the structure \"root/class/input\" then we can use `tf.keras.utils.image_dataset_from_directory`. But now we have an archive so we will create our own generator and use that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_tiny_mnist(archive_path, shuffle=True):\n",
    "    data = list(iter_archive(archive_path, extractfile=True, shuffle=shuffle))\n",
    "    \n",
    "    # Read\n",
    "    for filename, img in data:\n",
    "        label = tf.one_hot(\n",
    "            indices=int(filename.split('/')[1]) - 1,  # labels\n",
    "            depth=10,  # num classes\n",
    "        )\n",
    "        img = tf.convert_to_tensor(img[..., None])\n",
    "        yield img, label\n",
    "\n",
    "        \n",
    "batch_size = 128\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator=lambda: generate_tiny_mnist('data.tar'),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(10,10, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(10), dtype=tf.int32),\n",
    "    ),\n",
    ")\n",
    "dataset = dataset.repeat(3)\n",
    "dataset = dataset.shuffle(1024)  # might not be needed depending on if generator is called again for each epoch, I was unsure\n",
    "dataset = dataset.batch(128);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    keras.optimizers.Adam(learning_rate=0.01),\n",
    "    keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(dataset, steps_per_epoch=(1 + 60000 // batch_size), epochs=3, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    " 1. Make yourself acquainted with the above code.\n",
    " 2. In the future, if you have data on Mimer you can probably skip step 3.\n",
    " 3. Take a look at `jobscript-tensorflow.sh` in this script we will unpack the dataset on \\$TMPDIR and then train the model on the entire datase. Make sure not to unpack in your home folder as then you will exceed your file quota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using available datasets\n",
    "Some common public datasets are available at `/mimer/NOBACKUP/Datasets`, if there are some specific dataset you would like to see added you can create a request at [SNIC-support](https://supr.snic.se/support/).\n",
    "\n",
    "In this part we will access the MNIST dataset available at `/mimer/NOBACKUP/Datasets/MNIST/mnist.npz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 (3, 3) convolutional filters followed by a dense layer\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(10, 3, activation=\"relu\", input_shape=(28, 28, 1), use_bias=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we'll load the data has numpy arrays through the TensorFlow Keras backend. Then we'll massage this output into the correct shape. Another alternative would have been to use the TensorFlow Datasets API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_imgs, train_labels), _ = keras.datasets.mnist.load_data(path=\"/mimer/NOBACKUP/Datasets/MNIST/mnist.npz\")\n",
    "train_data = (\n",
    "    tf.expand_dims(train_imgs, 3),\n",
    "    tf.one_hot(train_labels, 10),\n",
    ")\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    keras.optimizers.Adam(learning_rate=0.01),\n",
    "    keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(dataset, steps_per_epoch=(1 + train_batches.n // train_batches.batch_size), epochs=3, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data through a TensorFlow related API\n",
    "Some datasets can be found and used through TensorFlow Keras as we did in the earlier example. The only difference is to change the path to were you would like to store the dataset. More datasets can be found through the [TensorFlow Datasets](https://www.tensorflow.org/datasets/overview), this package doesn't currently exist in the module tree but if interest exist it can probably be added.\n",
    "\n",
    "However, note that for both of these the data download can take some time and you will have to store them yourself. So for your and others sake please see if the datasets exist and for larger datasets don't hesitate to contact support if your are hesitant about anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
