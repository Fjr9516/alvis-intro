{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data with TensorFlow\n",
    "In this notebook we will investigate a few different ways to handle data with TensorFlow on Alvis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your own data\n",
    "In many cases you have a dataset in mind that you've already acquired and are keeping in your home folder or perhaps more probable in a storage project.\n",
    "\n",
    "In this section we will use the dataset in `data.tar.gz`, first let us take a look at it.\n",
    "\n",
    "### The file tree\n",
    "To see what is contained in a tar file the command `tar -tf my_tarfile.tar` is useful. However, we might want to specifically know some things about the directory structure and filenames. This is done in the below script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This will find the directories and files that do not have names ending with .png\n",
    "# and then count the number of files with names containing \".png\" for each of these\n",
    "echo \" #Files   Path\"\n",
    "echo \"==================\"\n",
    "for dir in $(tar --exclude=\"*.png\" -tf data.tar.gz); do\n",
    "    n_files=$(tar -tf data.tar.gz --wildcards \"$dir*.png\" | wc -l)\n",
    "    printf \"  %5s   %s\\n\" \"$n_files\" \"$dir\"\n",
    "done\n",
    "\n",
    "# List the 5 first and last png filenames in /data/1/\n",
    "echo  # New line\n",
    "echo \" Typical filenames\"\n",
    "echo \"===================\"\n",
    "tar -tf data.tar.gz --wildcards --no-anchored \"data/1/*.png\" | (head -n 5; echo \"...\"; tail -n 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:*** For this tar file there where \"only\" 60000 files, for archives that are much larger these operations will mean a significant FileIO and should be avoided as much as possible. If there is a README in connection with the dataset it is wise to take a look at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at some of the data\n",
    "Now we know the file structure of the data. Let us now get acquainted with the data a bit.\n",
    "\n",
    "First extract a small subset of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Extract the first 49 files\n",
    "tar -xvf data.tar.gz --wildcards \"data/*/im-000[0-4]?.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take a look at these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax_grid = plt.subplots(7, 7, figsize=(15, 15))\n",
    "for ix, ax in enumerate(ax_grid.flatten()):\n",
    "    # Get path to file and label\n",
    "    filepath = glob(f\"data/*/im-{ix:05d}.png\")[0]\n",
    "    _, label, filename = filepath.split(\"/\")\n",
    "    \n",
    "    # Add to axis\n",
    "    img = mpimg.imread(filepath)\n",
    "    ax.imshow(img)\n",
    "    # here I cheated because I already knew what the label meant\n",
    "    ax.set_title(f\"Digit {int(label) - 1}\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the labels are offset by 1 compared to the digits. The dataset is actually a modified version of the MNIST handwritten digit training database. The images have been shrunk to only 9x9 pixels and monochrome images to reduce the size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier from this data\n",
    "Now we have some understanding of what the database does and we are ready to do some ML on it.\n",
    "\n",
    "First we will define our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 (3, 3) convolutional filters followed by a dense layer\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(10, 3, activation=\"relu\", input_shape=(9, 9, 1), use_bias=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the step were we will load the data. When we have a dataset with the structure \"root/class/input\" then we can use `torchvision.dataset.DatasetFolder` or in the case of images `torchvision.dataset.ImageFolder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(glob(\"data/?/*.png\")) < 60000:\n",
    "    import warnings\n",
    "    warnings.warn(\"\\\"data/\\\" is not fully unpacked!\")\n",
    "\n",
    "img_path = '/cephyr/NOBACKUP/Datasets/tiny-imagenet-200/train'\n",
    "\n",
    "train_batches = ImageDataGenerator().flow_from_directory(\n",
    "    \"data\",\n",
    "    target_size=(9, 9),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataset = Dataset.from_generator(\n",
    "    lambda: train_batches,\n",
    "    output_types=(tf.float32, tf.float32), \n",
    "    output_shapes=([None, 9, 9, 1], [None, 10]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    keras.optimizers.Adam(learning_rate=0.01),\n",
    "    keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(dataset, steps_per_epoch=(1 + train_batches.n // train_batches.batch_size), epochs=3, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    " 1. Make yourself acquainted with the above code.\n",
    " 2. Take a look at `jobscript-tensorflow.sh` in this script we will unpack the dataset on \\$TMPDIR and then train the model on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using available datasets\n",
    "Some common public datasets are available at `/cephyr/NOBACKUP/Datasets`, if there are some specific dataset you would like to see added you can create a request at [SNIC-support](https://supr.snic.se/support/).\n",
    "\n",
    "In this part we will access the MNIST dataset available at `/cephyr/NOBACKUP/Datasets/MNIST/mnist.npz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 (3, 3) convolutional filters followed by a dense layer\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(10, 3, activation=\"relu\", input_shape=(28, 28, 1), use_bias=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we'll load the data has numpy arrays through the TensorFlow Keras backend. Then we'll massage this output into the correct shape. Another alternative would have been to use the TensorFlow Datasets API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_imgs, train_labels), _ = keras.datasets.mnist.load_data(path=\"/cephyr/NOBACKUP/Datasets/MNIST/mnist.npz\")\n",
    "train_data = (\n",
    "    tf.expand_dims(train_imgs, 3),\n",
    "    tf.one_hot(train_labels, 10),\n",
    ")\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    keras.optimizers.Adam(learning_rate=0.01),\n",
    "    keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(dataset, steps_per_epoch=(1 + train_batches.n // train_batches.batch_size), epochs=3, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data through a TensorFlow related API\n",
    "Some datasets can be found and used through TensorFlow Keras as we did in the earlier example. The only difference is to change the path to were you would like to store the dataset. More datasets can be found through the [TensorFlow Datasets](https://www.tensorflow.org/datasets/overview), this package doesn't currently exist in the module tree but if interest exist it can probably be added.\n",
    "\n",
    "However, note that for both of these the data download can take some time and you will have to store them yourself. So for your and others sake please see if the datasets exist and for larger datasets don't hesitate to contact support if your are hesitant about anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
