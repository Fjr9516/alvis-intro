{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing with PyTorch\n",
    "In this notebook we will go through checkpointing your model with PyTorch.\n",
    "\n",
    "## Setting up model and dataset\n",
    "For this example we will use [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet/overview) which is similar to ImageNet but lower resolution (64x64) and fewer images (100 k). For this dataset we will use a variant of the ResNet architecture wich is a type of Convolutional Neural Network with residual connections. For the sake of this tutorial you do not need to understand the details about the model or the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we move the dataset to TMPDIR if one is available\n",
    "import os\n",
    "\n",
    "if \"TMPDIR\" in os.environ:\n",
    "    data_path = os.path.join(os.environ[\"TMPDIR\"], \"tiny-imagenet-200/\")\n",
    "    if not os.path.isdir(data_path):\n",
    "        !cp \"/cephyr/NOBACKUP/Datasets/tiny-imagenet-200/tiny-imagenet-200.zip\" \"$TMPDIR\"\n",
    "        !unzip -n \"$TMPDIR/tiny-imagenet-200.zip\" -d \"$TMPDIR\"\n",
    "else:\n",
    "    data_path = \"/cephyr/NOBACKUP/Datasets/tiny-imagenet-200\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, layers, Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNetIterator(DirectoryIterator):\n",
    "    '''Help class when loading TinyImageNet.'''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        parent_directory,\n",
    "        subset,\n",
    "        image_data_generator,\n",
    "        target_size=(64, 64),\n",
    "        color_mode='rgb',\n",
    "        classes=None,\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=None,\n",
    "        data_format='channels_last',\n",
    "        save_to_dir=None,\n",
    "        save_prefix='',\n",
    "        save_format='png',\n",
    "        follow_links=False,\n",
    "        interpolation='nearest',\n",
    "        dtype='float32',\n",
    "    ):\n",
    "        train_directory = os.path.join(parent_directory, \"train\")\n",
    "        if subset==\"training\":\n",
    "            return super().__init__(\n",
    "                train_directory,\n",
    "                image_data_generator,\n",
    "                target_size=target_size,\n",
    "                color_mode=color_mode,\n",
    "                classes=classes,\n",
    "                class_mode=class_mode,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                seed=seed,\n",
    "                data_format=data_format,\n",
    "                save_to_dir=save_to_dir,\n",
    "                save_prefix=save_prefix,\n",
    "                save_format=save_format,\n",
    "                follow_links=follow_links,\n",
    "                subset=subset,\n",
    "                interpolation=interpolation,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        elif subset==\"validation\":\n",
    "            directory = os.path.join(parent_directory, \"val\")\n",
    "        else:\n",
    "            raise ValueError(f'Value of subset should be \"training\" or \"validation\",  not {subset}.')\n",
    "        \n",
    "        # Modified Directory Iterator __init__\n",
    "        super(DirectoryIterator, self).set_processing_attrs(\n",
    "            image_data_generator=image_data_generator,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            data_format=data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format,\n",
    "            subset=subset,\n",
    "            interpolation=interpolation,\n",
    "        )\n",
    "        self.directory = directory\n",
    "        self.classes = classes\n",
    "        if class_mode not in self.allowed_class_modes:\n",
    "            raise ValueError(f'Invalid class_mode: {class_mode}; expected one of: {self.allowed_class_modes}')\n",
    "        self.class_mode = class_mode\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # First, count the number of samples and classes.\n",
    "        class_names = classes\n",
    "        if not class_names:\n",
    "            class_names = []\n",
    "            for subdir in sorted(os.listdir(train_directory)):\n",
    "                if os.path.isdir(os.path.join(train_directory, subdir)):\n",
    "                    class_names.append(subdir)\n",
    "        self.num_classes = len(class_names)\n",
    "        self.class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "\n",
    "        # Get map between filename and class index                 \n",
    "        with open(os.path.join(directory, \"val_annotations.txt\"), \"r\") as f:\n",
    "            filenames, classes = zip(*[\n",
    "                (fn, self.class_indices[class_name])\n",
    "                for fn, class_name, _, _, _, _\n",
    "                in csv.reader(f, delimiter=\"\\t\")\n",
    "            ])\n",
    "\n",
    "        self.filenames = filenames\n",
    "        self.samples = len(self.filenames)\n",
    "        self.classes = np.array(classes, dtype='int32')\n",
    "\n",
    "        print(f'Found {self.samples} images belonging to {self.num_classes} classes.')\n",
    "        self._filepaths = [os.path.join(self.directory, fn) for fn in self.filenames]\n",
    "        grandparent_class = self.__class__.__mro__[2]  # sorry, not nice code\n",
    "        super(grandparent_class, self).__init__(self.samples, batch_size, shuffle, seed)        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size + 1\n",
    "        \n",
    "\n",
    "class TinyImageNetGenerator(ImageDataGenerator, TinyImageNetIterator):\n",
    "    \n",
    "    \n",
    "    def __bool__(self):\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def flow_from_directory(\n",
    "        self,\n",
    "        parent_directory,\n",
    "        subset,\n",
    "        *,\n",
    "        target_size=(64, 64),\n",
    "        color_mode=\"rgb\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return TinyImageNetIterator(\n",
    "            parent_directory,\n",
    "            subset,\n",
    "            self,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 10000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "dir_parent = '/cephyr/NOBACKUP/Datasets/tiny-imagenet-200/'\n",
    "\n",
    "train_batches = TinyImageNetGenerator().flow_from_directory(dir_parent, \"training\", batch_size=128)\n",
    "val_batches = TinyImageNetGenerator().flow_from_directory(dir_parent, \"validation\", batch_size=128)\n",
    "\n",
    "train_set = Dataset.from_generator(\n",
    "    lambda: train_batches,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 64, 64, 3], [None, 200])\n",
    ")\n",
    "val_set = Dataset.from_generator(\n",
    "    lambda: val_batches,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 64, 64, 3], [None, 200])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self, filters, strides=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.relu = layers.ReLU()\n",
    "        self.conv1 = layers.Conv2D(filters, 3, strides=strides, padding=\"same\", use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-5)\n",
    "        self.conv2 = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization(epsilon=1e-5)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        prev_shape = x.shape\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        identity = inputs if self.downsample is None else self.downsample(inputs)\n",
    "    \n",
    "        return self.relu(x + identity)\n",
    "\n",
    "\n",
    "class ResNet(keras.Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers,\n",
    "        num_classes=1000,\n",
    "        zero_init_residual=False,\n",
    "        groups=1,\n",
    "        downsample=None,\n",
    "        name=\"resnet\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.block = ResidualBlock\n",
    "        \n",
    "        self.in_filters = 64\n",
    "        self.dilation = 1\n",
    "        self.groups = 1\n",
    "        \n",
    "        # Defining layers\n",
    "        self.relu = layers.ReLU()\n",
    "        self.conv1 = layers.Conv2D(filters=64, kernel_size=7, strides=2, padding=\"same\", use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-5)\n",
    "        self.maxpool = layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\")\n",
    "        self.layer1 = self._make_layer(64, n_layers[0])\n",
    "        self.layer2 = self._make_layer(128, n_layers[1], strides=2)\n",
    "        self.layer3 = self._make_layer(256, n_layers[2], strides=2)\n",
    "        self.layer4 = self._make_layer(512, n_layers[3], strides=2)\n",
    "        self.avgpool = layers.AveragePooling2D(pool_size=1)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "    \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, layers.Conv2D):\n",
    "                layer.kernel_initializer = keras.initializers.VarianceScaling(\n",
    "                    scale=2.0,\n",
    "                    mode=\"fan_out\",\n",
    "                )        \n",
    "    \n",
    "    \n",
    "    def _make_layer(self, filters, n_blocks, strides=1):\n",
    "        block = self.block\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        check_singular_strides = lambda strides: (tuple(strides) != (1, 1) if isinstance(strides, Iterable) else strides != 1)\n",
    "        if check_singular_strides(strides) or self.in_filters != filters:\n",
    "            downsample = keras.Sequential([\n",
    "                layers.Conv2D(filters, 1, strides=strides, use_bias=False),\n",
    "                layers.BatchNormalization(epsilon=1e-5),\n",
    "            ])\n",
    "        \n",
    "        layer = keras.Sequential()\n",
    "        layer.add(block(filters, strides=strides, downsample=downsample))\n",
    "        self.in_filters = filters\n",
    "        for _ in range(1, n_blocks):\n",
    "            layer.add(block(filters))\n",
    "    \n",
    "        return layer\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet18 = ResNet([2, 2, 2, 2], num_classes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the important part, the training. In this part we will have to include the checkpointing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  3/782 [..............................] - ETA: 35:21 - loss: 6.8403 - accuracy: 0.0000e+00 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-c12f8e2fcdd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.5.0-fosscuda-2020b/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.5.0-fosscuda-2020b/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.5.0-fosscuda-2020b/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.5.0-fosscuda-2020b/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.5.0-fosscuda-2020b/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.5.0-fosscuda-2020b/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/Alvis/software/TensorFlow/2.5.0-fosscuda-2020b/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet18.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "resnet18.fit(train_set, epochs=5, steps_per_epoch=len(train_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from checkpoint\n",
    "Now that we have created a checkpointed we want to load it to check how it performs against the validation set again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=False, num_classes=200)\n",
    "checkpoint = torch.load(\"checkpoint.pt\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = validate(model)\n",
    "print(f'''\n",
    "Validation loss: {loss:.4f}\n",
    "Accuracy:        {acc:.4f}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "1. Write a `train_from_checkpoint` function below that given the path to a checkpoint continues training from there\n",
    "2. Modify the `train_from_checkpoint` function to also save the best checkpoint so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
