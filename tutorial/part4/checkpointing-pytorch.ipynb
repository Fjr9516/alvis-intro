{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing with PyTorch\n",
    "In this notebook we will go through checkpointing your model with PyTorch.\n",
    "\n",
    "## Setting up model and dataset\n",
    "For this example we will use [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet/overview) which is similar to ImageNet but lower resolution (64x64) and fewer images (100 k). For this dataset we will use a variant of the ResNet architecture wich is a type of Convolutional Neural Network with residual connections. For the sake of this tutorial you do not need to understand the details about the model or the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we move the dataset to TMPDIR if one is available\n",
    "import os\n",
    "\n",
    "if \"TMPDIR\" in os.environ:\n",
    "    data_path = os.path.join(os.environ[\"TMPDIR\"], \"tiny-imagenet-200/\")\n",
    "    if not os.path.isdir(data_path):\n",
    "        !cp \"/cephyr/NOBACKUP/Datasets/tiny-imagenet-200/tiny-imagenet-200.zip\" \"$TMPDIR\"\n",
    "        !unzip -n \"$TMPDIR/tiny-imagenet-200.zip\" -d \"$TMPDIR\"\n",
    "else:\n",
    "    data_path = \"/cephyr/NOBACKUP/Datasets/tiny-imagenet-200\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class TinyImageNet(ImageFolder):\n",
    "    '''The directory structure of val is a bit off.\n",
    "    \n",
    "    To fix so that this works with the validation set\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, root_parent, type, *args, **kwargs):\n",
    "        self.type = type\n",
    "        self.root_parent = root_parent\n",
    "        super().__init__(os.path.join(self.root_parent, self.type), *args, **kwargs)\n",
    "    \n",
    "    def make_dataset(self, directory, class_to_idx, *args, **kwargs):\n",
    "        \"\"\"Generates a list of samples of a form (path_to_sample, class).\"\"\"\n",
    "        if class_to_idx is None:\n",
    "            raise ValueError(\"The parameter class_to_idx cannot be None.\")\n",
    "        \n",
    "        if self.type == \"train\":\n",
    "            return super().make_dataset(directory, class_to_idx, *args, **kwargs)\n",
    "        \n",
    "        with open(os.path.join(self.root, self.type + \"_annotations.txt\"), \"r\") as f:\n",
    "            return [\n",
    "                (os.path.join(self.root, \"images\", fn), class_to_idx[class_name])\n",
    "                for fn, class_name, _, _, _, _\n",
    "                in csv.reader(f, delimiter=\"\\t\")\n",
    "            ]\n",
    "    \n",
    "    def _find_classes(self, directory):\n",
    "        \"\"\"List of all classes and dictionary mapping each class to an index.\"\"\"\n",
    "        train_dir = os.path.join(directory, \"..\", \"train\")\n",
    "        return super()._find_classes(train_dir)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "val_set    = TinyImageNet(data_path, \"val\",   transform=transform)\n",
    "train_set  = TinyImageNet(data_path, \"train\", transform=transform)\n",
    "\n",
    "load_kws = dict(\n",
    "    num_workers = 4,\n",
    "    batch_size = 512,\n",
    "    prefetch_factor = 512,\n",
    ")\n",
    "val_loader   = DataLoader(val_set,   shuffle=False, **load_kws)\n",
    "train_loader = DataLoader(train_set, shuffle=True,  **load_kws)\n",
    "\n",
    "# ResNet-18\n",
    "pretrained = True\n",
    "model = resnet18(pretrained=False, num_classes=200)\n",
    "if pretrained:\n",
    "    # If we like we can use weights trained on ImageNet 1000\n",
    "    pretrained_state_dict = resnet18(pretrained=pretrained, num_classes=1000).state_dict()\n",
    "    # However, the last fully connected layer is the wrong shape    \n",
    "    for key in [\"fc.weight\", \"fc.bias\"]:\n",
    "        del pretrained_state_dict[key]\n",
    "    model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "\n",
    "# Optimizer\n",
    "opt = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the important part, the training. In this part we will have to include the checkpointing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def train(model, opt, n_epochs, checkpoint_path, device=device):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    total_steps = n_epochs * n_batches\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Training epoch\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            \n",
    "            est = model(images)\n",
    "            \n",
    "            loss = loss_func(est, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            counter += 1\n",
    "            print(f\"\\rProgress: {100 * counter / total_steps:4.1f} %  ({counter}/{total_steps})\", end=\"\")\n",
    "            \n",
    "        train_loss /= n_batches\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = validate(model, device=device)\n",
    "        print(f\"\\rEpoch {epoch}, Train loss {train_loss}, Val loss {val_loss}, Val acc {val_acc}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": opt.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        \n",
    "def validate(model, device=device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0.0\n",
    "        n_batches = len(val_loader)\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            est = model(images)\n",
    "            loss += loss_func(est, labels).item()\n",
    "            acc = (labels == est.argmax(1)).float().mean().item()\n",
    "        \n",
    "        loss /= n_batches\n",
    "        \n",
    "        return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss 3.5107031768682053, Val loss 2.3952553033828736, Val acc 0.4632352888584137\n",
      "Epoch 1, Train loss 2.0370431670120785, Val loss 2.0661282539367676, Val acc 0.5367646813392639\n",
      "Epoch 2, Train loss 1.6109773054414867, Val loss 1.9544510900974275, Val acc 0.5551470518112183\n",
      "Epoch 3, Train loss 1.2980858890377744, Val loss 1.9074746668338776, Val acc 0.5588235259056091\n",
      "Epoch 4, Train loss 1.0297039169438031, Val loss 1.9163922369480133, Val acc 0.5808823704719543\n",
      "CPU times: user 3min 3s, sys: 1min 10s, total: 4min 14s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model, opt, 5, checkpoint_path=\"checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from checkpoint\n",
    "Now that we have created a checkpointed we want to load it to check how it performs against the validation set again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18(pretrained=False, num_classes=200)\n",
    "checkpoint = torch.load(\"checkpoint.pt\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 1.9164\n",
      "Accuracy:        0.5809\n"
     ]
    }
   ],
   "source": [
    "loss, acc = validate(model)\n",
    "print(f'''\n",
    "Validation loss: {loss:.4f}\n",
    "Accuracy:        {acc:.4f}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "1. Write a `train_from_checkpoint` function below that given the path to a checkpoint continues training from there\n",
    "2. Modify the `train_from_checkpoint` function to also save the best checkpoint so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
