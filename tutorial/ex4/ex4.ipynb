{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using custom datasets \n",
    "\n",
    "In this tutorial, we will show how to use multiple GPUs to speed up an intensive PyTorch training application using a custom datasets. We implement a slightly modified version of the PyTorch GPU benchmark which uses fake image data: <https://github.com/ryujaehun/pytorch-gpu-benchmark>.\n",
    "\n",
    "Execute the following cell to setup the environment. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "import platform,psutil\n",
    "import time,os\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create your custom dataset by inheritting from the `torch.Dataset` class. As you can see, we use random numbers here representing fake images."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class RandomDataset(Dataset):\n",
    "\n",
    "    def __init__(self,  length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn( 3, 224, 224,length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[:,:,:,index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We investigate the ResNet model here and pick only one member of this family, the `resnext101_32x8d`. The particular choice is irrelevent for the purpose of this tutorial. You can experiment with other models included in the original source.\n",
    "\n",
    "This test is run in float- and single-precision mode only. You can include `'double'` in your tests as well, but be aware of the substantial time that it will take to run the benchmark."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Uncomment the following line to get a list of all models within the ResNet family\n",
    "# print(models.resnet.__all__)\n",
    "\n",
    "MODEL_LIST = { models.resnet: ['resnext101_32x8d'] } \n",
    "precisions=[\"float\",\"half\"] # \"double\" will take a substantial amount of time!\n",
    "\n",
    "device_name=str(torch.cuda.get_device_name(0))\n",
    "BATCH_SIZE=64\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "WARM_UP = 5   # Num of warm up runs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have a local directory with your own image data, you can use `data = datasets.ImageFolder(root=YOUR_DATA_FOLDER)`. Here, we use `DataLoader` to load the data in batches. Remember that it also supports loading data in parallel using `torch.multiprocessing`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "NUM_TEST = 50   # Num of Test\n",
    "trainloader = DataLoader(dataset=RandomDataset( BATCH_SIZE*(WARM_UP + NUM_TEST)),\n",
    "                         batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, \n",
    "                         num_workers=2)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images  = dataiter.next()\n",
    "print(images.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 3, 224, 224])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def train(type='float'):\n",
    "    \"\"\"use fake image for training speed test\"\"\"\n",
    "    target = torch.LongTensor(BATCH_SIZE).random_(1000).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    benchmark = {}\n",
    "    for model_type in MODEL_LIST.keys():\n",
    "        for model_name in MODEL_LIST[model_type]:\n",
    "            model = getattr(model_type, model_name)(pretrained=False)\n",
    "            if gpu_count > 1:\n",
    "                model = nn.DataParallel(model,device_ids=range(gpu_count))\n",
    "            model=getattr(model,type)()\n",
    "            model=model.to('cuda')\n",
    "            durations = []\n",
    "            print('Benchmarking Training {} precision type {} '.format(type,model_name))\n",
    "            for step,img in enumerate(trainloader):                \n",
    "                img=getattr(img,type)()\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.time()\n",
    "                model.zero_grad()\n",
    "                prediction = model(img.to('cuda'))\n",
    "                loss = criterion(prediction, target)\n",
    "                loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "                if step >= WARM_UP:\n",
    "                    durations.append((end - start)*1000)\n",
    "            print(model_name,' model average train time : ',sum(durations)/len(durations),'ms')\n",
    "            del model\n",
    "            benchmark[model_name] = durations\n",
    "    return benchmark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def inference(type='float'):\n",
    "    benchmark = {}\n",
    "    with torch.no_grad():\n",
    "        for model_type in MODEL_LIST.keys():\n",
    "            for model_name in MODEL_LIST[model_type]:\n",
    "                model = getattr(model_type, model_name)(pretrained=False)\n",
    "                if gpu_count > 1:\n",
    "                    model = nn.DataParallel(model,device_ids=range(gpu_count))\n",
    "                model=getattr(model,type)()\n",
    "                model=model.to('cuda')\n",
    "                model.eval()\n",
    "                durations = []\n",
    "                print('Benchmarking Inference {} precision type {} '.format(type,model_name))\n",
    "                for step,img in enumerate(trainloader):\n",
    "                    img=getattr(img,type)()\n",
    "                    torch.cuda.synchronize()\n",
    "                    start = time.time()\n",
    "                    model(img.to('cuda'))\n",
    "                    torch.cuda.synchronize()\n",
    "                    end = time.time()\n",
    "                    if step >= WARM_UP:\n",
    "                        durations.append((end - start)*1000)\n",
    "                print(model_name,' model average inference time : ',sum(durations)/len(durations),'ms')\n",
    "                del model\n",
    "                benchmark[model_name] = durations\n",
    "    return benchmark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell will run the model and print some benchmark results. In a test run with 3x`T4` vs. 3x`V100` GPUs, we obtained the following results for both float and half-precision cases:\n",
    "\n",
    "\n",
    "|           | 3xT4   | 3xV100 |\n",
    "|-----------|--------|--------|\n",
    "| *Float*   |        |        |\n",
    "| Training  | 1046.8 | 352.8  |\n",
    "| Inference | 352.9  | 131.1  |\n",
    "| *Half*    |        |        |\n",
    "| Training  | 543.8  | 217.1  |\n",
    "| Inference | 172.3  | 70.6   |\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "if __name__ == '__main__':\n",
    "    folder_name='new_results'\n",
    "    path=''\n",
    "    device_name=\"\".join((device_name, '_',str(gpu_count),'_gpus_'))\n",
    "    system_configs=str(platform.uname())\n",
    "    system_configs='\\n'.join((system_configs,str(psutil.cpu_freq()),'cpu_count: '+str(psutil.cpu_count()),'memory_available: '+str(psutil.virtual_memory().available)))\n",
    "    gpu_configs=[torch.cuda.device_count(),torch.version.cuda,torch.backends.cudnn.version(),torch.cuda.get_device_name(0)]\n",
    "    gpu_configs=list(map(str,gpu_configs))\n",
    "    temp=['Number of GPUs on current device : ','CUDA Version : ','Cudnn Version : ','Device Name : ']\n",
    "\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    now = time.localtime()\n",
    "    start_time=str(\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "    print('benchmark start : ',start_time)\n",
    "\n",
    "    for idx,value in enumerate(zip(temp,gpu_configs)):\n",
    "        gpu_configs[idx]=''.join(value)\n",
    "        print(gpu_configs[idx])\n",
    "    print(system_configs)\n",
    "\n",
    "    with open(os.path.join(folder_name,\"system_info.txt\"), \"w\") as f:\n",
    "        f.writelines('benchmark start : '+start_time+'\\n')\n",
    "        f.writelines('system_configs\\n\\n')\n",
    "        f.writelines(system_configs)\n",
    "        f.writelines('\\ngpu_configs\\n\\n')\n",
    "        f.writelines(s + '\\n' for s in gpu_configs )\n",
    "\n",
    "    \n",
    "    for precision in precisions:\n",
    "        train_result=train(precision)\n",
    "        train_result_df = pd.DataFrame(train_result)\n",
    "        path=''.join((folder_name,'/',device_name,\"_\",precision,'_model_train_benchmark.csv'))\n",
    "        train_result_df.to_csv(path, index=False)\n",
    "\n",
    "        inference_result=inference(precision)\n",
    "        inference_result_df = pd.DataFrame(inference_result)\n",
    "        path=''.join((folder_name,'/',device_name,\"_\",precision,'_model_inference_benchmark.csv'))\n",
    "        inference_result_df.to_csv(path, index=False)\n",
    "\n",
    "    now = time.localtime()\n",
    "    end_time=str(\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "    print('benchmark end : ',end_time)\n",
    "    with open(os.path.join(folder_name,\"system_info.txt\"), \"a\") as f:\n",
    "        f.writelines('benchmark end : '+end_time+'\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "benchmark start :  2020/12/07 16:28:04\n",
      "Number of GPUs on current device : 3\n",
      "CUDA Version : 10.1\n",
      "Cudnn Version : 7604\n",
      "Device Name : Tesla V100-SXM2-32GB\n",
      "uname_result(system='Linux', node='alvis1-17', release='3.10.0-1160.6.1.el7.x86_64', version='#1 SMP Tue Nov 17 13:59:11 UTC 2020', machine='x86_64', processor='x86_64')\n",
      "scpufreq(current=1905.1988437499997, min=1200.0, max=3900.0)\n",
      "cpu_count: 32\n",
      "memory_available: 770921381888\n",
      "Benchmarking Training float precision type resnext101_32x8d \n",
      "resnext101_32x8d  model average train time :  338.517689704895 ms\n",
      "Benchmarking Inference float precision type resnext101_32x8d \n",
      "resnext101_32x8d  model average inference time :  115.7274866104126 ms\n",
      "Benchmarking Training half precision type resnext101_32x8d \n",
      "resnext101_32x8d  model average train time :  221.17517948150635 ms\n",
      "Benchmarking Inference half precision type resnext101_32x8d \n",
      "resnext101_32x8d  model average inference time :  75.29011726379395 ms\n",
      "benchmark end :  2020/12/07 16:28:51\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}