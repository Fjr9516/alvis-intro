{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing with TensorFlow\n",
    "In this notebook we will go through checkpointing your model with TensorFlow.\n",
    "\n",
    "## Setting up model and dataset\n",
    "For this example we will use [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet/overview) which is similar to ImageNet but lower resolution (64x64) and fewer images (100 k). For this dataset we will use a variant of the ResNet architecture which is a type of Convolutional Neural Network with residual connections. For the sake of this tutorial you do not need to understand the details about the model or the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we move the dataset to TMPDIR if one is available\n",
    "import os\n",
    "\n",
    "if \"TMPDIR\" in os.environ:\n",
    "    data_path = os.path.join(os.environ[\"TMPDIR\"], \"tiny-imagenet-200/\")\n",
    "    if not os.path.isdir(data_path):\n",
    "        !cp -r \"/mimer/NOBACKUP/Datasets/tiny-imagenet-200\" -d \"$TMPDIR\"\n",
    "else:\n",
    "    data_path = \"/mimer/NOBACKUP/Datasets/tiny-imagenet-200\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 14:21:55.578251: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, layers, Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNetIterator(DirectoryIterator):\n",
    "    '''Help class when loading TinyImageNet.'''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        parent_directory,\n",
    "        subset,\n",
    "        image_data_generator,\n",
    "        target_size=(64, 64),\n",
    "        color_mode='rgb',\n",
    "        classes=None,\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=None,\n",
    "        data_format='channels_last',\n",
    "        save_to_dir=None,\n",
    "        save_prefix='',\n",
    "        save_format='png',\n",
    "        follow_links=False,\n",
    "        interpolation='nearest',\n",
    "        dtype='float32',\n",
    "    ):\n",
    "        train_directory = os.path.join(parent_directory, \"train\")\n",
    "        if subset==\"training\":\n",
    "            return super().__init__(\n",
    "                train_directory,\n",
    "                image_data_generator,\n",
    "                target_size=target_size,\n",
    "                color_mode=color_mode,\n",
    "                classes=classes,\n",
    "                class_mode=class_mode,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                seed=seed,\n",
    "                data_format=data_format,\n",
    "                save_to_dir=save_to_dir,\n",
    "                save_prefix=save_prefix,\n",
    "                save_format=save_format,\n",
    "                follow_links=follow_links,\n",
    "                subset=subset,\n",
    "                interpolation=interpolation,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        elif subset==\"validation\":\n",
    "            directory = os.path.join(parent_directory, \"val\")\n",
    "        else:\n",
    "            raise ValueError(f'Value of subset should be \"training\" or \"validation\",  not {subset}.')\n",
    "        \n",
    "        # Modified Directory Iterator __init__\n",
    "        super(DirectoryIterator, self).set_processing_attrs(\n",
    "            image_data_generator=image_data_generator,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            data_format=data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format,\n",
    "            subset=subset,\n",
    "            interpolation=interpolation,\n",
    "        )\n",
    "        self.directory = directory\n",
    "        self.classes = classes\n",
    "        if class_mode not in self.allowed_class_modes:\n",
    "            raise ValueError(f'Invalid class_mode: {class_mode}; expected one of: {self.allowed_class_modes}')\n",
    "        self.class_mode = class_mode\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # First, count the number of samples and classes.\n",
    "        class_names = classes\n",
    "        if not class_names:\n",
    "            class_names = []\n",
    "            for subdir in sorted(os.listdir(train_directory)):\n",
    "                if os.path.isdir(os.path.join(train_directory, subdir)):\n",
    "                    class_names.append(subdir)\n",
    "        self.num_classes = len(class_names)\n",
    "        self.class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "\n",
    "        # Get map between filename and class index                 \n",
    "        with open(os.path.join(directory, \"val_annotations.txt\"), \"r\") as f:\n",
    "            filenames, classes = zip(*[\n",
    "                (os.path.join(\"images\", fn), self.class_indices[class_name])\n",
    "                for fn, class_name, _, _, _, _\n",
    "                in csv.reader(f, delimiter=\"\\t\")\n",
    "            ])\n",
    "\n",
    "        self.filenames = filenames\n",
    "        self.samples = len(self.filenames)\n",
    "        self.classes = np.array(classes, dtype='int32')\n",
    "\n",
    "        print(f'Found {self.samples} images belonging to {self.num_classes} classes.')\n",
    "        self._filepaths = [os.path.join(self.directory, fn) for fn in self.filenames]\n",
    "        grandparent_class = self.__class__.__mro__[2]  # sorry, not nice code\n",
    "        super(grandparent_class, self).__init__(self.samples, batch_size, shuffle, seed)        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size + 1\n",
    "        \n",
    "\n",
    "class TinyImageNetGenerator(ImageDataGenerator, TinyImageNetIterator):\n",
    "    \n",
    "    \n",
    "    def __bool__(self):\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def flow_from_directory(\n",
    "        self,\n",
    "        parent_directory,\n",
    "        subset,\n",
    "        *,\n",
    "        target_size=(64, 64),\n",
    "        color_mode=\"rgb\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return TinyImageNetIterator(\n",
    "            parent_directory,\n",
    "            subset,\n",
    "            self,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BatchFromFilesMixin.set_processing_attrs() missing 1 required positional argument: 'keep_aspect_ratio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      4\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m TinyImageNetGenerator()\u001b[38;5;241m.\u001b[39mflow_from_directory(dir_parent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m----> 5\u001b[0m val_batches \u001b[38;5;241m=\u001b[39m \u001b[43mTinyImageNetGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_parent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m train_set \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: train_batches,\n\u001b[1;32m      9\u001b[0m     output_types\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mfloat32, tf\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     10\u001b[0m     output_shapes\u001b[38;5;241m=\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m200\u001b[39m])\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m val_set \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: val_batches,\n\u001b[1;32m     14\u001b[0m     output_types\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mfloat32, tf\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     15\u001b[0m     output_shapes\u001b[38;5;241m=\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m200\u001b[39m])\n\u001b[1;32m     16\u001b[0m )\n",
      "Cell \u001b[0;32mIn [3], line 117\u001b[0m, in \u001b[0;36mTinyImageNetGenerator.flow_from_directory\u001b[0;34m(self, parent_directory, subset, target_size, color_mode, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    110\u001b[0m     parent_directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    116\u001b[0m ):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTinyImageNetIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [3], line 51\u001b[0m, in \u001b[0;36mTinyImageNetIterator.__init__\u001b[0;34m(self, parent_directory, subset, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, interpolation, dtype)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue of subset should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,  not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Modified Directory Iterator __init__\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDirectoryIterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_processing_attrs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_data_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_data_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory \u001b[38;5;241m=\u001b[39m directory\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m classes\n",
      "\u001b[0;31mTypeError\u001b[0m: BatchFromFilesMixin.set_processing_attrs() missing 1 required positional argument: 'keep_aspect_ratio'"
     ]
    }
   ],
   "source": [
    "dir_parent = data_path\n",
    "\n",
    "batch_size = 512\n",
    "train_batches = TinyImageNetGenerator().flow_from_directory(dir_parent, \"training\", batch_size=batch_size)\n",
    "val_batches = TinyImageNetGenerator().flow_from_directory(dir_parent, \"validation\", batch_size=batch_size)\n",
    "\n",
    "train_set = Dataset.from_generator(\n",
    "    lambda: train_batches,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 64, 64, 3], [None, 200])\n",
    ")\n",
    "val_set = Dataset.from_generator(\n",
    "    lambda: val_batches,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 64, 64, 3], [None, 200])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self, filters, strides=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.relu = layers.ReLU()\n",
    "        self.conv1 = layers.Conv2D(filters, 3, strides=strides, padding=\"same\", use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-5)\n",
    "        self.conv2 = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization(epsilon=1e-5)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        prev_shape = x.shape\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        identity = inputs if self.downsample is None else self.downsample(inputs)\n",
    "    \n",
    "        return self.relu(x + identity)\n",
    "\n",
    "\n",
    "class ResNet(keras.Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers,\n",
    "        num_classes=1000,\n",
    "        zero_init_residual=False,\n",
    "        groups=1,\n",
    "        downsample=None,\n",
    "        name=\"resnet\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.block = ResidualBlock\n",
    "        \n",
    "        self.in_filters = 64\n",
    "        self.dilation = 1\n",
    "        self.groups = 1\n",
    "        \n",
    "        # Defining layers\n",
    "        self.relu = layers.ReLU()\n",
    "        self.conv1 = layers.Conv2D(filters=64, kernel_size=7, strides=2, padding=\"same\", use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-5)\n",
    "        self.maxpool = layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\")\n",
    "        self.layer1 = self._make_layer(64, n_layers[0])\n",
    "        self.layer2 = self._make_layer(128, n_layers[1], strides=2)\n",
    "        self.layer3 = self._make_layer(256, n_layers[2], strides=2)\n",
    "        self.layer4 = self._make_layer(512, n_layers[3], strides=2)\n",
    "        self.avgpool = layers.AveragePooling2D(pool_size=1)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "    \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, layers.Conv2D):\n",
    "                layer.kernel_initializer = keras.initializers.VarianceScaling(\n",
    "                    scale=2.0,\n",
    "                    mode=\"fan_out\",\n",
    "                )        \n",
    "    \n",
    "    \n",
    "    def _make_layer(self, filters, n_blocks, strides=1):\n",
    "        block = self.block\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        check_singular_strides = lambda strides: (tuple(strides) != (1, 1) if isinstance(strides, Iterable) else strides != 1)\n",
    "        if check_singular_strides(strides) or self.in_filters != filters:\n",
    "            downsample = keras.Sequential([\n",
    "                layers.Conv2D(filters, 1, strides=strides, use_bias=False),\n",
    "                layers.BatchNormalization(epsilon=1e-5),\n",
    "            ])\n",
    "        \n",
    "        layer = keras.Sequential()\n",
    "        layer.add(block(filters, strides=strides, downsample=downsample))\n",
    "        self.in_filters = filters\n",
    "        for _ in range(1, n_blocks):\n",
    "            layer.add(block(filters))\n",
    "    \n",
    "        return layer\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = ResNet([2, 2, 2, 2], num_classes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the important part, the training. In this part we will have to include the checkpointing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing is done via callback\n",
    "checkpoint_path = \"checkpoints-tf/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,  # will not save entire model\n",
    "    mode='auto',\n",
    "    save_freq='epoch',\n",
    "    options=None,\n",
    ")\n",
    "\n",
    "# Compile model as usual\n",
    "resnet18.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Remember to add checkpoint callback\n",
    "resnet18.fit(\n",
    "    train_set,\n",
    "    epochs=5,\n",
    "    steps_per_epoch=len(train_batches),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    validation_data=val_set,\n",
    "    validation_steps=len(val_batches),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the above run (using 8 epochs) that we get the expect single checkpoint per epoch.\n",
    "\n",
    "(As a side note, the results can be significantly improved if trained from a pretrained ResNet that is available from torchvision, but converting weights from PyTorch is a bit out of scope for this tutorial.)\n",
    "\n",
    "In this example we decided to only save weight during checkpointing but we can also save the entire model. Here we do it with the trained model in the SavedModel format (instead of hdf5 which is the other alternative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resnet18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresnet18\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-tf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resnet18' is not defined"
     ]
    }
   ],
   "source": [
    "resnet18.save(\"model-tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the different directory structures of checkpointing and saving teh model separetely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tree checkpoints-tf\n",
    "\n",
    "tree model-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that in addition to the saved models we also get meta data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from checkpoint\n",
    "Now that we have created a checkpointed we want to load it to and I've also added a check to see that the loading went as planned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_model = ResNet([2, 2, 2, 2], num_classes=200)\n",
    "ckpt_model.compile(\n",
    "    optimizer=keras.optimizers.SGD(),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "latest_ckpt = tf.train.latest_checkpoint(\"checkpoints-tf\")\n",
    "ckpt_model.load_weights(latest_ckpt)\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('model-tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in zip(range(len(val_batches)), val_set):\n",
    "    y_saved  = resnet18(x)\n",
    "    y_ckpt   = ckpt_model(x)\n",
    "    y_loaded = loaded_model(x)\n",
    "    \n",
    "    tf.debugging.assert_near(y_saved, y_ckpt)\n",
    "    tf.debugging.assert_near(y_ckpt, y_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "1. Create a cell below that can continue continues training from the latest checkpoint\n",
    "2. Modify the training also save the best model so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
